import logging
from collections import defaultdict
from typing import Any, List

import torch
import torch.nn as nn
from omegaconf import DictConfig

from src.algorithms.algorithm_base import AlgorithmBase
from src.utils import CompileParams

log = logging.getLogger(__name__)


class ERM(AlgorithmBase):
    
    def __init__(
        self,
        compiled: CompileParams,
        groups_count: int,
        allowable_errors: list[int],
        optimizer_conf: DictConfig = None,
        scheduler_conf: DictConfig = None,
        network: nn.Module = None,
        network_conf: DictConfig = None,
        gpu: int = None,
        *args, **kwargs
    ):
        super().__init__(
            compiled=compiled,
            optimizer_conf=optimizer_conf,
            scheduler_conf=scheduler_conf,
            network=network,
            network_conf=network_conf,
            gpu=gpu
        )
        
        self._groups_count = groups_count  # TODO do we really need this or we can mine this info from the data
        self._allowable_errors = allowable_errors
        
        self._mse__ = nn.MSELoss()
        self._mse = nn.MSELoss(reduction='none')
        self._group_weights = self.__get_initial_group_weights()  # sets (1/n, 1/n, 1/n, ..., 1/n)
        
        self.training_step_outputs = []
        self.validation_step_outputs = defaultdict(list)
        self.test_step_outputs = defaultdict(list)
    
    def pred(self, batch):
        sequence, _, ue_location, image_size, is_los = batch
        ue_location_pred = self._network(torch.Tensor([sequence]).cuda(self._gpu))
        return ue_location_pred
    
    def _step(self, batch, *args, **kwargs):
        sequence, _, ue_location, image_size, is_los = batch
        ue_location_pred = self._network(sequence)
        return self.get_metrics(ue_location, image_size, is_los, ue_location_pred)
    
    def on_train_batch_end(self, outputs, batch: Any, batch_idx: int) -> None:
        outputs = AlgorithmBase.convert_to_numpy(outputs)
        self.training_step_outputs.append(outputs)
    
    def on_validation_batch_end(self, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:
        outputs = AlgorithmBase.convert_to_numpy(outputs)
        self.validation_step_outputs[dataloader_idx].append(outputs)
    
    def on_test_batch_end(self, outputs, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:
        outputs = AlgorithmBase.convert_to_numpy(outputs)
        self.test_step_outputs[dataloader_idx].append(outputs)
    
    def get_metrics(self, ue_location, image_size, is_los, ue_location_pred):
        mses = self._mse(ue_location_pred, ue_location[:, :2]).sum(dim=1).sqrt()
        mses_meters = mses * image_size
        
        accuracies = {f"acc_{p}": (mses_meters < p).sum() / len(mses_meters) for p in self._allowable_errors}
        
        mse = self._mse__(ue_location_pred, ue_location[:, :2])
        mse_meters = mses_meters.mean()
        
        group_metrics = self._get_group_metrics(is_los, mses_meters, self._allowable_errors)
        
        loss = mse
        
        metrics = {
            'loss': loss,
            **{acc: acc_val.to('cpu').detach() for acc, acc_val in accuracies.items()},
            'mse_meters': mse_meters.to('cpu').detach(),
            
            "group_metrics": group_metrics
        }
        
        return metrics
    
    def __get_initial_group_weights(self):
        group_weights = torch.ones(self._groups_count)
        group_weights = group_weights / group_weights.sum()
        group_weights = group_weights.to('cpu')
        
        return group_weights
    
    def _get_group_metrics(self, z, mses_meters, allowable_errors: list[int]):
        group_metrics = {}
        for group_i in range(self._groups_count):
            group_mses_meters = mses_meters[z == group_i]
            group_count = len(group_mses_meters)
            
            group_mse_meters = 0 if group_count == 0 else group_mses_meters.mean().to('cpu').detach()
            
            group_accuracies = {
                f"acc_{p}": 0 if group_count == 0 else (group_mses_meters < p).sum().to('cpu').detach() / group_count
                for p in allowable_errors
            }
            
            group_metrics = {
                **group_metrics,
                group_i: {
                    "metrics": {
                        f"mse_meters": group_mse_meters,
                        **group_accuracies
                    },
                    "count": len(group_mses_meters)
                }
            }
        
        return group_metrics
    
    def _calculate_epoch_metrics(self, outputs: List[Any]) -> dict:
        general_metric_names = [k for k in outputs[0].keys() if k != "group_metrics"]
        group_metric_names = [k for k in outputs[0]["group_metrics"][0]["metrics"].keys()]
        
        # init combined metrics with zero values
        combined_general_metrics = {k: 0 for k in general_metric_names}
        
        combined_group_metrics = {
            i: dict(
                metrics=dict(**{metric: 0 for metric in group_metric_names}),
                count=0
            )
            for i in range(self._groups_count)
        }
        
        # add all output values to combined_group_metrics
        for o in outputs:
            for group_i, group_metrics_obj in o['group_metrics'].items():
                for metric_name, metric_val in group_metrics_obj["metrics"].items():
                    combined_group_metrics[group_i]['metrics'][metric_name] += metric_val * group_metrics_obj["count"]
                combined_group_metrics[group_i]["count"] += group_metrics_obj["count"]
            
            for k in o.keys():
                if k != "group_metrics":
                    combined_general_metrics[k] += o[k]
        
        # compute means of metrics
        for group_i, group_metrics_obj in combined_group_metrics.items():
            for metric_name, metric_val in group_metrics_obj["metrics"].items():
                combined_group_metrics[group_i]['metrics'][metric_name] /= (
                    combined_group_metrics[group_i]["count"] + 1e-6
                )
        
        for k in outputs[0].keys():
            if k != "group_metrics":
                combined_general_metrics[k] /= len(outputs)
        
        # worst case info calculation
        worst_case_info = {k: [] for k in combined_group_metrics[0]["metrics"].keys()}
        for group_metrics_obj in combined_group_metrics.values():
            for metric_name, metric_val in group_metrics_obj["metrics"].items():
                worst_case_info[metric_name].append(metric_val)
        
        worst_case_info = {
            f"worst_group_{k}": (min(vals) if "acc" in k else max(vals))
            for k, vals in worst_case_info.items()
        }
        
        # merge group metrics into one dict
        combined_group_metrics = {
            f"group_{i}_{metric_name}": metric_val for i, m in combined_group_metrics.items()
            for metric_name, metric_val in {**m["metrics"], "count": m["count"]}.items()
        }
        
        # merge all
        epoch_metrics_sep = {
            **combined_general_metrics,
            **combined_group_metrics,
            **worst_case_info
        }
        
        epoch_metrics_shared = {
            "learning_rate": self.trainer.optimizers[0].param_groups[0]["lr"]
        }
        for i in range(self._groups_count):
            epoch_metrics_sep[f"w_group_{i}"] = self._group_weights[i]
        
        if self.logger:
            self.logger.log_metrics(epoch_metrics_shared, self.trainer.current_epoch)
        else:
            log.info(f"""\n{epoch_metrics_shared}\n""")
        
        return epoch_metrics_sep
